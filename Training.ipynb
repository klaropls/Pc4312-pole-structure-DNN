{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53956cd0-04e9-46f8-9a3c-ea0843e82b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from dnn_models.ipynb\n",
      "importing Jupyter notebook from DNN_model_final.ipynb\n",
      "importing Jupyter notebook from final_pole_module.ipynb\n",
      "Number of poles to be generated per class: 9000000\n",
      "Ndata to be generated= 36000000\n",
      "Your directory is: curriculum02_training\n"
     ]
    }
   ],
   "source": [
    "import chainer\n",
    "from chainer import configuration\n",
    "from chainer.dataset import convert\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import optimizers, initializers, serializers\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "import random\n",
    "import os\n",
    "\n",
    "import sklearn\n",
    "from sklearn.utils import shuffle\n",
    "import pickle\n",
    "\n",
    "import import_ipynb\n",
    "import dnn_models\n",
    "from dnn_models import MLP1L, MLP2L, MLP3L, MLP4L, MLP5L\n",
    "import DNN_model_final\n",
    "from DNN_model_final import CustomMLP\n",
    "\n",
    "import final_pole_module\n",
    "from final_pole_module import detect_cusp, unif_pole, T1, T2, Einput\n",
    "from final_pole_module import mu1, mu2, T1, T2, T4, Nreal, Nimag, hbarc\n",
    "from final_pole_module import Einput, Ereal, Eimag, Erealfar, Eimagfar, E_exp, labelz, inspect, NEpoints\n",
    "from final_pole_module import skip_duplicate, export_data, import_data, get_traintest, directory\n",
    "\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b372b47b-74b6-4433-80f2-319a26e4f58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchsize = 1600\n",
    "# max_epoch = 3000\n",
    "# directory = 'curriculum01_training'\n",
    "# resume = False\n",
    "\n",
    "gpu_id = 0\n",
    "device = chainer.get_device(gpu_id)\n",
    "\n",
    "# num_arch = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35fd73d5-026e-4ba8-87f1-3031e677b827",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Choose curricululum to train\n",
    "curriculum = 3 ###curriculum19 na aq ngaun. change 512*2 batchsize\n",
    "\n",
    "resume0 = True\n",
    "\n",
    "resume1 = True\n",
    "#Set resume1 to False if you want to continue using main directory\n",
    "#Set resume1 to True if you want to resume using snapshot directory\n",
    "\n",
    "resume2 = False\n",
    "#Set resume2 to True if you want to continue using PREVIOUS CURRICULUM snapshot directory\n",
    "#Set resume2 to False otherwise\n",
    "\n",
    "#Which last epoch would you like to continue?\n",
    "last_epoch = 2985 #31070 #31050\n",
    "#Set minibatch size\n",
    "batchsize = 64 #128 1-10 11-21 #256\n",
    "\n",
    "#For resume=False. If you want to initialize weights (I am not sure if this is truly effective)\n",
    "initialize = True\n",
    "\n",
    "#Set maximum epoch of the full training\n",
    "max_epoch = 3000 - last_epoch #3200 - last_epoch\n",
    "#Set maximum repetitions during epoch restart\n",
    "max_rep = 1000\n",
    "#Set accuracy drop tolerance to execute epoch restart\n",
    "drop_tolerance = -0.75 #-0.005 #-0.5 #-0.005 #set to -0.5 for first ten epoch of new curriculum\n",
    "#Save model and state every save_epoch\n",
    "save_epoch = 5\n",
    "\n",
    "#Choose your DNN model\n",
    "dnn1 = CustomMLP(7,[300,400,500,300,200,100,20]) #MLP1L() #CustomMLP(2,[200,100])\n",
    "\n",
    "#local directory and file name of training and testing curriculum dataset\n",
    "#trainset = 'curriculum_trainset//chainer_train_curr{:02d}.pkl'.format(curriculum)\n",
    "#testset = 'curriculum_testset//chainer_test_curr{:02d}.pkl'.format(curriculum)\n",
    "curriculum_folder = 'curriculum{:02d}_training'.format(curriculum)\n",
    "trainset = os.path.join(curriculum_folder, 'chainer_train_curr{:02d}.pkl'.format(curriculum))\n",
    "testset = os.path.join(curriculum_folder, 'chainer_test_curr{:02d}.pkl'.format(curriculum))\n",
    "\n",
    "#Continuous training directory\n",
    "directory1 = 'SMORMS_7diamond1_retest_3currs1000each_curr{:02d}_full'.format(curriculum)\n",
    "#Snapshot directory (save models every save_epoch)\n",
    "directory2 = 'SMORMS_7diamond1_retest_3currs1000each_curr{:02d}_snapshot'.format(curriculum)\n",
    "\n",
    "#gpu_id = 0\n",
    "\n",
    "#Load previous training results from the snapshot folder\n",
    "prev_curr = curriculum\n",
    "if curriculum > 1:\n",
    "    prev_curr = curriculum - 1\n",
    "\n",
    "out = directory1\n",
    "if not os.path.isdir(out):\n",
    "    os.makedirs(out)\n",
    "\n",
    "out2 = directory2\n",
    "if not os.path.isdir(out2):\n",
    "    os.makedirs(out2)\n",
    "\n",
    "#present curriculum directory\n",
    "if resume1 == True and resume2 == False:\n",
    "    directory3 = directory2\n",
    "#previous curriculum directory\n",
    "if resume1 == True and resume2 == True:\n",
    "    directory3 = 'SMORMS_7diamond1_retest_3currs1000each_curr{:02d}_snapshot'.format(prev_curr)\n",
    "\n",
    "    \n",
    "if resume1 == True:        \n",
    "    import shutil\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//MLP3.model'.format(last_epoch)),directory1)\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//MLP3.state'.format(last_epoch)),directory1)\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//testing_accu1.pkl'.format(last_epoch)),directory1)\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//testing_loss1.pkl'.format(last_epoch)),directory1)\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//training_accu1.pkl'.format(last_epoch)),directory1)\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//training_loss1.pkl'.format(last_epoch)),directory1)\n",
    "    shutil.copy(os.path.join(directory3,'epoch{:06d}//epoch_log.txt'.format(last_epoch)),directory1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d13f602-6f78-45e4-9abf-b8566f85da04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dnn():\n",
    "    #GPU usage is only activated here.\n",
    "    \n",
    "    device = chainer.get_device(gpu_id)\n",
    "    model1 = L.Classifier(dnn1)\n",
    "    model1.to_device(device)\n",
    "    device.use()\n",
    "    \n",
    "    optimizer1 = chainer.optimizers.SMORMS3() \n",
    "    #alpha=0.001, weight_decay_rate =0 default amsgrad=True, adabound=True\n",
    "    optimizer1 = optimizer1.setup(model1)\n",
    "    \n",
    "    # Weight initilizer\n",
    "    if resume0 == False and resume1 == False and initialize == True:\n",
    "        chainer.initializers.HeNormal(model1)\n",
    "    \n",
    "    #Load training and testing datasets\n",
    "    train = pickle.load(open(trainset,'rb'))\n",
    "    test = pickle.load(open(testset,'rb'))\n",
    "    \n",
    "    #Define Iterator\n",
    "    train_iter = chainer.iterators.SerialIterator(train, batchsize)\n",
    "    test_iter = chainer.iterators.SerialIterator(test, batchsize, repeat=False, shuffle=False)\n",
    "    \n",
    "    out = directory1\n",
    "    if not os.path.isdir(out):\n",
    "        os.makedirs(out)\n",
    "\n",
    "    out2 = directory2\n",
    "    if not os.path.isdir(out2):\n",
    "        os.makedirs(out2)        \n",
    "        \n",
    "    if resume0 == True:\n",
    "        #Load log book\n",
    "        log = open(os.path.join(out,'epoch_log.txt'),'a+')\n",
    "        #Load lists of training and testing accuracies of the previous run \n",
    "        training_accu1 = pickle.load(open(os.path.join(out,\"training_accu1.pkl\"),\"rb\"))\n",
    "        testing_accu1 = pickle.load(open(os.path.join(out,\"testing_accu1.pkl\"),\"rb\"))\n",
    "        #Load lists of traininf and testing losses of the previous run \n",
    "        training_loss1 = pickle.load(open(os.path.join(out,\"training_loss1.pkl\"),\"rb\"))\n",
    "        testing_loss1 = pickle.load(open(os.path.join(out,\"testing_loss1.pkl\"),\"rb\"))\n",
    "        #Load the model and the optimizer of the previous run\n",
    "        serializers.load_npz(os.path.join(out,\"MLP3.model\"), model1)\n",
    "        serializers.load_npz(os.path.join(out,\"MLP3.state\"), optimizer1)\n",
    "                                     \n",
    "        #I need this to assign value to last epoch. Batch size might change and we cannot rely on the iterators\n",
    "        training_accu1 = pickle.load(open(os.path.join(out,\"training_accu1.pkl\"),\"rb\"))\n",
    "        \n",
    "    elif resume0 == False:\n",
    "        log = open(os.path.join(out,'epoch_log.txt'),'w+')\n",
    "        log = open(os.path.join(out,'epoch_log.txt'),'a+')\n",
    "        #Initialize lists of losses and accuracies\n",
    "        training_loss1 = []\n",
    "        training_accu1 = []\n",
    "        testing_loss1 = []\n",
    "        testing_accu1 = []\n",
    "        \n",
    "        #I need this to assign value to last epoch\n",
    "        training_accu1 = [] \n",
    "    \n",
    "    last_epoch = len(training_accu1)\n",
    "        \n",
    "    \n",
    "    time_start = time.time()    \n",
    "    test_count = len(test)\n",
    "    \n",
    "    #Initialize training\n",
    "    train_count = 0\n",
    "    \n",
    "    sum1_loss = 0\n",
    "    sum1_accu = 0\n",
    "    \n",
    "    rep = 0\n",
    "    #---------------------------start training epoch----------------------------------------------\n",
    "    while (train_iter.epoch < max_epoch) and (rep < max_rep):\n",
    "        batch = train_iter.next()\n",
    "        x, t = convert.concat_examples(batch, device)\n",
    "        train_count += len(t)\n",
    "        #Update network's parameters using forward pass and backpropagation for each model\n",
    "        optimizer1.update(model1, x, t)\n",
    "        #Calculate training loss and accuracy\n",
    "        sum1_loss += float(model1.loss.array)*len(t)\n",
    "        sum1_accu += float(model1.accuracy.array)*len(t)\n",
    "    #-----------------------end of one epoch----------------------------------------------------\n",
    "        \n",
    "        if train_iter.is_new_epoch:\n",
    "            #Record training loss and accuracy for each model\n",
    "            training_loss1.append(sum1_loss/train_count)\n",
    "            training_accu1.append(sum1_accu/train_count)\n",
    "            #Initialize loss and accuracy for testing\n",
    "            sum1_loss = 0\n",
    "            sum1_accu = 0\n",
    "            \n",
    "            #Enable evaluation mode\n",
    "            with configuration.using_config('train', False):\n",
    "                #This is optional but can reduce computational overhead\n",
    "                with chainer.using_config('enable_backprop', False):\n",
    "                    for batch in test_iter:\n",
    "                        x, t = convert.concat_examples(batch, device)\n",
    "                        #Calculate testing loss and accuracy\n",
    "                        loss1 = model1(x,t)\n",
    "                        sum1_loss += float(loss1.array)*len(t)\n",
    "                        sum1_accu += (model1.accuracy.array)*len(t)\n",
    "                        \n",
    "            test_iter.reset()\n",
    "            \n",
    "            #I need this to assign value to the recent epoch\n",
    "            epoch = len(training_accu1)            \n",
    "            \n",
    "            #Record testing loss and accuracy\n",
    "            testing_loss1.append(sum1_loss/test_count)\n",
    "            testing_accu1.append(sum1_accu/test_count)\n",
    "            \n",
    "            #Restart if training is too bad\n",
    "            drop = float(training_accu1[len(training_accu1)-1]) - float(training_accu1[len(training_accu1)-2])\n",
    "            retrain = 0\n",
    "            if drop < drop_tolerance:\n",
    "                retrain = retrain + 1\n",
    "                rep = retrain\n",
    "                #Load log book\n",
    "                log = open(os.path.join(out,'epoch_log.txt'),'a+')\n",
    "                log.write('We are restarting at epoch {:06d} for {:03d} times \\r\\n'.format(epoch, retrain))\n",
    "                #Load lists of training and testing accuracies of the previous run \n",
    "                training_accu1 = pickle.load(open(os.path.join(out,\"training_accu1.pkl\"),\"rb\"))\n",
    "                testing_accu1 = pickle.load(open(os.path.join(out,\"testing_accu1.pkl\"),\"rb\"))\n",
    "                #Load lists of training and testing losses of the previous run \n",
    "                training_loss1 = pickle.load(open(os.path.join(out,\"training_loss1.pkl\"),\"rb\"))\n",
    "                testing_loss1 = pickle.load(open(os.path.join(out,\"testing_loss1.pkl\"),\"rb\"))\n",
    "                #Load the model and the optimizer of the previous run\n",
    "                serializers.load_npz(os.path.join(out,\"MLP3.model\"), model1)\n",
    "                serializers.load_npz(os.path.join(out,\"MLP3.state\"), optimizer1)\n",
    "                #I need this to assign value to the recent epoch\n",
    "                epoch = len(training_accu1)\n",
    "                \n",
    "            \n",
    "            #Save model and optimizer state\n",
    "            serializers.save_npz(os.path.join(out,\"MLP3.model\"), model1)\n",
    "            serializers.save_npz(os.path.join(out,\"MLP3.state\"), optimizer1)\n",
    "            \n",
    "            #Save training data\n",
    "            pickle.dump(training_loss1,open(os.path.join(out,\"training_loss1.pkl\"),\"wb\"))\n",
    "            pickle.dump(training_accu1,open(os.path.join(out,\"training_accu1.pkl\"),\"wb\"))\n",
    "            \n",
    "            #Save testing data\n",
    "            pickle.dump(testing_loss1,open(os.path.join(out,\"testing_loss1.pkl\"),\"wb\"))\n",
    "            pickle.dump(testing_accu1,open(os.path.join(out,\"testing_accu1.pkl\"),\"wb\"))\n",
    "\n",
    "            time_epoch = time.time() - time_start\n",
    "            log.write('epoch:{:06d} time elapsed:{:0.06f} sec \\r\\n'.format(epoch, time_epoch))\n",
    "            \n",
    "            log.flush()\n",
    "            print('epoch:{:06d} done time elapsed:{:0.06f} sec'.format(epoch, time_epoch))            \n",
    "            \n",
    "            #Reinitialize for next training\n",
    "            sum1_loss = 0\n",
    "            sum1_accu = 0                \n",
    "            train_count = 0\n",
    "            \n",
    "            #Time machine: if something goes wrong, you can always go back\n",
    "            if epoch%save_epoch == 0:\n",
    "                out3 = 'epoch{:06d}'.format(epoch)\n",
    "                if not os.path.isdir(os.path.join(out2, out3)):\n",
    "                    os.mkdir(os.path.join(out2, out3))\n",
    "                    \n",
    "                import shutil\n",
    "                shutil.copy(os.path.join(out,'MLP3.model'),os.path.join(out2,out3))\n",
    "                shutil.copy(os.path.join(out,'MLP3.state'),os.path.join(out2,out3))\n",
    "                shutil.copy(os.path.join(out,'testing_accu1.pkl'),os.path.join(out2,out3))\n",
    "                shutil.copy(os.path.join(out,'testing_loss1.pkl'),os.path.join(out2,out3))\n",
    "                shutil.copy(os.path.join(out,'training_accu1.pkl'),os.path.join(out2,out3))\n",
    "                shutil.copy(os.path.join(out,'training_loss1.pkl'),os.path.join(out2,out3))\n",
    "                shutil.copy(os.path.join(out,'epoch_log.txt'),os.path.join(out2,out3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e19f4a5-e17d-42b5-9ea7-7dafdb0c0bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c219c-5b87-4366-b6fe-50d8dd8af237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb88232-b6ec-4abb-bd51-909c0e40bb27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "a9a36b86-11f2-4d6e-b5f6-46444978cf22",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
